{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ML1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Initialization](#Initialization)\n",
    "- [Motivation and Background](#Motivation-and-Background)\n",
    "- [Data Basics](#Data-Basics)\n",
    "- [Understanding the Data](#Understanding-the-Data)\n",
    "\n",
    "    - [_Exercise 1 - descriptive statistics_](#Exercise-1---descriptive-statistics)\n",
    "\n",
    "- [Cleaning and Subsetting Data](#Cleaning-and-Subsetting-Data)\n",
    "\n",
    "    - [_Exercise 2 - function `cleanData`_](#Exercise-2---function-cleanData)\n",
    "    - [Checkpoint - save `cleaned_data_frame`](#Checkpoint---save-cleaned_data_frame)\n",
    "\n",
    "- [Model Selection and Assessment](#Model-Selection-and-Assessment)\n",
    "\n",
    "    - [Cleanup - clear `cleaned_data_frame`](#Cleanup---clear-cleaned_data_frame)\n",
    "    - [Training a model](#Training-a-model)\n",
    "    - [1. Split data into training and testing data sets](#1.-Split-data-into-training-and-testing-data-sets)\n",
    "    - [Cleanup - clear X and y data frames](#Cleanup---clear-X-and-y-data-frames)\n",
    "    - [2. Model Build - Fit model](#2.-Model-Build---Fit-model)\n",
    "    - [3. Evaluate Performance - accuracy](#3.-Evaluate-Performance---accuracy)\n",
    "\n",
    "- [_Exercise 3 - Train, fit and evaluate your model_](#Exercise-3---Train,-fit-and-evaluate-your-model)\n",
    "\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Initialization\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Before we begin, run the code cell below to initialize the libraries we'll be using in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sqlalchemy import create_engine\n",
    "import numpy\n",
    "from IPython.core.display import Image\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import gc\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ML2",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Motivation and Background\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Research in science policy often involves making use of publically available datasets.  Today, we will be diving into the National Institutes for Health (NIH) grant data.  NIH provides a myriad of information about its grants which you can access here: http://projectreporter.nih.gov/reporter.cfm. Unfortunately, since the information draws on disparate sources like eRA databases, Medline, PubMed Central, the NIH Intramural Database, and iEdison, it is often not complete. \n",
    "\n",
    "In this workbook, we will examine one application of machine learning that deals with predicting missing information. Often in sciece policy research, we are interested in knowing what areas of science are being funded.  There are diverse techniques to determine a grant's area of science, including assessing a grants topic using text analytics like topic modeling (covered in the Text Analysis workbook).\n",
    "\n",
    "The best option is an explicit and standardized identifier that is present on every record.  The NIH grant data doesn't have an area of science taxonomy.  It does, however, provide a place to store an expicit, though potentially not standardized or totally accurate, indicator of a grant's overall topic area - the academic department of the grant's primary investigator (PI).  Unfortunately, this variable is often left empty.  In this workbook, we will walk through the process of imputing values for a missing categorical variable using the example of predicting the academic department of a given grant's primary investigator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ML3",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Data Basics\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The data we need for this exercise is in the '`umetricsgrants`' database in MySQL on the class server.  For more details on this data, see the data schemas we've provided in class:\n",
    "\n",
    "- Database Schema PowerPoint (also includes USPTO and StarMetrics data) - [http://jpsmonline.umd.edu/mod/resource/view.php?id=2387](http://jpsmonline.umd.edu/mod/resource/view.php?id=2387)\n",
    "- UmetricsGrants Schema (image) - [http://jpsmonline.umd.edu/mod/resource/view.php?id=2384](http://jpsmonline.umd.edu/mod/resource/view.php?id=2384)\n",
    "\n",
    "In particular, in the '`nih_project`' table, there is a variable '`ORG_DEPT`' that details the department of the PI of the grant.  This will be our outcome variable of interest (the variable we will try to predict when it is missing).  Once you start trying your own models, you can use as predictors variables from any of the tables whose names start with '`nih_`'.  To get us started, however, we have taken a subset of columns from these tables and placed them in a single table named '`MachineLearning2`' in the '`homework`' database.\n",
    "\n",
    "In the past (in the Database Basics and Text Analysis assignment notebooks) we have interacted with the database using SQL and a direct python connection to the database.  In this lesson, we'll be using a different program - the **pandas** package ( site: [http://pandas.pydata.org/](http://pandas.pydata.org/); doc: [http://pandas.pydata.org/pandas-docs/stable/index.html](http://pandas.pydata.org/pandas-docs/stable/index.html) ) - to read in and manipulate data.  Pandas provides an alternative to reading data directly from MySQL that stores the data in special table format called a \"data frame\" that allows for easy statistical analysis and can be directly used for machine learning.\n",
    "\n",
    "Pandas uses a database engine to connect to databases (via the SQLAlchemy Python package).  In the code cell below, we will create a database engine conneted to our class MySQL database server for Pandas to use.  In the code cell below, place your database username and password in the variables '`mysql_username`' and '`mysql_password`', then run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up database credentials\n",
    "mysql_username = \"root\"\n",
    "mysql_password = \"password\"\n",
    "\n",
    "# Create database connection for pandas.\n",
    "pandas_db = create_engine( \"mysql+pymysql://\" + mysql_username + \":\" + mysql_password + \"@localhost:3306/homework?charset=utf8\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we will use this database connection to have pandas read in the data stored in the '`MachineLearning2`' table.  Pandas has a set of Input/Output tools that let it read from and write to a large variety of tabular data formats ( [http://pandas.pydata.org/pandas-docs/stable/io.html](http://pandas.pydata.org/pandas-docs/stable/io.html) ), including CSV and Excel files, databases via SQL, JSON files, and SAS and Stata data files.  In the example below, we'll use the `pandas.read_sql()` function to read the results of an SQL query into a pandas data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_frame = pandas.read_sql( 'Select * from homework.MachineLearning2;', pandas_db )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets look at what the data looks like.  The DataFrame method '`data_frame.head( number_of_rows )`' outputs the  first `number_of_rows` rows in a data frame.  Lets look at the first five rows in our data.\n",
    "\n",
    "In the code cell below, there are two ways to output this information.  If you just call the method, you'll get an HTML table output directly into the ipython notebook.  If you pass the results of the method to the \"`print()`\" function, you'll get text output that works outside of jupyter/ipython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_ID</th>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>ACTIVITY</th>\n",
       "      <th>ADMINISTERING_IC</th>\n",
       "      <th>ARRA_FUNDED</th>\n",
       "      <th>ORG_NAME</th>\n",
       "      <th>ORG_DEPT</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>STUDY_SECTION</th>\n",
       "      <th>TOTAL_COST</th>\n",
       "      <th>ED_INST_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6169887</td>\n",
       "      <td>856</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>R01</td>\n",
       "      <td>AI</td>\n",
       "      <td>\u0000</td>\n",
       "      <td>UNIVERSITY OF NORTH CAROLINA CHAPEL HILL</td>\n",
       "      <td>PUBLIC HEALTH &amp;PREV MEDICINE</td>\n",
       "      <td>874</td>\n",
       "      <td>EVR</td>\n",
       "      <td>201232.0</td>\n",
       "      <td>SCHOOLS OF PUBLIC HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6642672</td>\n",
       "      <td>859</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>R01</td>\n",
       "      <td>GM</td>\n",
       "      <td>\u0000</td>\n",
       "      <td>UNIVERSITY OF ILLINOIS AT CHICAGO</td>\n",
       "      <td>CHEMISTRY</td>\n",
       "      <td>435</td>\n",
       "      <td>BNP</td>\n",
       "      <td>126325.0</td>\n",
       "      <td>SCHOOLS OF ARTS AND SCIENCES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6650290</td>\n",
       "      <td>837</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>R01</td>\n",
       "      <td>HL</td>\n",
       "      <td>\u0000</td>\n",
       "      <td>UNIVERSITY OF PITTSBURGH AT PITTSBURGH</td>\n",
       "      <td>PUBLIC HEALTH &amp;PREV MEDICINE</td>\n",
       "      <td>919</td>\n",
       "      <td>EDC</td>\n",
       "      <td>555615.0</td>\n",
       "      <td>SCHOOLS OF PUBLIC HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7211854</td>\n",
       "      <td>855</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>R01</td>\n",
       "      <td>AI</td>\n",
       "      <td>\u0000</td>\n",
       "      <td>UNIVERSITY OF CALIFORNIA LOS ANGELES</td>\n",
       "      <td>MICROBIOLOGY/IMMUN/VIROLOGY</td>\n",
       "      <td>888</td>\n",
       "      <td>PTHE</td>\n",
       "      <td>382112.0</td>\n",
       "      <td>SCHOOLS OF MEDICINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6326438</td>\n",
       "      <td>279</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>R01</td>\n",
       "      <td>DA</td>\n",
       "      <td>\u0000</td>\n",
       "      <td>EMORY UNIVERSITY</td>\n",
       "      <td>NONE</td>\n",
       "      <td>643</td>\n",
       "      <td>ZRG1</td>\n",
       "      <td>477174.0</td>\n",
       "      <td>SCHOOLS OF PUBLIC HEALTH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   APPLICATION_ID CFDA_CODE    YEAR ACTIVITY ADMINISTERING_IC ARRA_FUNDED  \\\n",
       "0         6169887       856  2000.0      R01               AI           \u0000   \n",
       "1         6642672       859  2002.0      R01               GM           \u0000   \n",
       "2         6650290       837  2003.0      R01               HL           \u0000   \n",
       "3         7211854       855  2007.0      R01               AI           \u0000   \n",
       "4         6326438       279  2001.0      R01               DA           \u0000   \n",
       "\n",
       "                                   ORG_NAME                      ORG_DEPT  \\\n",
       "0  UNIVERSITY OF NORTH CAROLINA CHAPEL HILL  PUBLIC HEALTH &PREV MEDICINE   \n",
       "1         UNIVERSITY OF ILLINOIS AT CHICAGO                     CHEMISTRY   \n",
       "2    UNIVERSITY OF PITTSBURGH AT PITTSBURGH  PUBLIC HEALTH &PREV MEDICINE   \n",
       "3      UNIVERSITY OF CALIFORNIA LOS ANGELES   MICROBIOLOGY/IMMUN/VIROLOGY   \n",
       "4                          EMORY UNIVERSITY                          NONE   \n",
       "\n",
       "   topic_id STUDY_SECTION  TOTAL_COST                  ED_INST_TYPE  \n",
       "0       874           EVR    201232.0      SCHOOLS OF PUBLIC HEALTH  \n",
       "1       435           BNP    126325.0  SCHOOLS OF ARTS AND SCIENCES  \n",
       "2       919           EDC    555615.0      SCHOOLS OF PUBLIC HEALTH  \n",
       "3       888          PTHE    382112.0           SCHOOLS OF MEDICINE  \n",
       "4       643          ZRG1    477174.0      SCHOOLS OF PUBLIC HEALTH  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a pretty tabular view, just call the method.\n",
    "data_frame.head( 5 )\n",
    "\n",
    "# to get a text-based view, print() the call to the method.\n",
    "#print( data_frame.head( 5 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Understanding the Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In pandas, our data is represented by a DataFrame. You can think of data frames as a giant spreadsheet which you can program, with the data for each column stored in its own list that pandas calls a Series (or vector of values), along with a set of methods (another name for functions that are tied to objects) that make managing data in pandas easy.\n",
    "\n",
    "A Series is a list of values each of which can also have a label, which pandas calls an \"index\", and which generally is used to store names of columns when you retrieve a Series that represents a row, and IDs of rows when you retrieve a Series that represents a column of data in a table.\n",
    "\n",
    "While DataFrames and Series are separate objects, they may share the same methods where those methods make sense in both a table and list context (`head()` and `tail()`, as used in examples in this notebook, for example).\n",
    "\n",
    "More details on pandas data structures:\n",
    "\n",
    "- Data Structures overview: [http://pandas.pydata.org/pandas-docs/stable/dsintro.html](http://pandas.pydata.org/pandas-docs/stable/dsintro.html)\n",
    "- Series specifics: [http://pandas.pydata.org/pandas-docs/stable/dsintro.html#series](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#series)\n",
    "- DataFrame specifics: [http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe)\n",
    "\n",
    "For example, if you want to look at the last five values in the `ORG_DEPT` column, you can retrieve the Series that contains the `ORG_DEPT` column's data using square-bracket notation (like you'd use to access a value in a dictionary):\n",
    "\n",
    "    data_frame[ \"ORG_DEPT\" ]\n",
    "    \n",
    "then call the \"`tail( number_of_rows )`\" method (the opposite of \"`head()`\") on the Series to get `number_of_rows` values from the end of the column's data.  To get the last 5 values in the `ORG_DEPT` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99995           NEUROSCIENCES\n",
      "99996            PHARMACOLOGY\n",
      "99997    OTHER BASIC SCIENCES\n",
      "99998              PSYCHIATRY\n",
      "99999            PHARMACOLOGY\n",
      "Name: ORG_DEPT, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vector of \"ORG_DEPT\" column values from data frame\n",
    "org_dept_column_series = data_frame[ \"ORG_DEPT\" ]\n",
    "\n",
    "# see the last 5 values in the vector.\n",
    "print( org_dept_column_series.tail( 5 ) )\n",
    "\n",
    "# It is also OK to chain together, but I did not above for clarity's sake, and in\n",
    "#    general, be wary of doing too many things on one line.\n",
    "# data_frame[ \"ORG_DEPT\" ].tail( 5 )\n",
    "\n",
    "# empty org_dept_column_series variable and garbage collect, to conserve memory\n",
    "org_dept_column_series = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the data is stored internally, we can reference the \"`data_frame.dtypes`\" variable, which contains a pandas Series object with the name of each column in your data frame the label for the type of the data in that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPLICATION_ID        int64\n",
       "CFDA_CODE            object\n",
       "YEAR                float64\n",
       "ACTIVITY             object\n",
       "ADMINISTERING_IC     object\n",
       "ARRA_FUNDED          object\n",
       "ORG_NAME             object\n",
       "ORG_DEPT             object\n",
       "topic_id              int64\n",
       "STUDY_SECTION        object\n",
       "TOTAL_COST          float64\n",
       "ED_INST_TYPE         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets look at these database columns one by one:\n",
    "\n",
    "* **`APPLICATION_ID`** - Unique identifier for each grant\n",
    "* **`CFDA_CODE`** - CFDA contains detailed program descriptions for 2,292 Federal assistance programs: [https://www.cfda.gov/](https://www.cfda.gov/)\n",
    "* **`YEAR`** - Year in which grant was awarded\n",
    "* **`ACTIVITY`** - A 3-character code identifying the grant, contract, or intramural activity through which a project is supported. Here is a list of activity codes: [http://grants.nih.gov/grants/funding/ac_search_results.htm](http://grants.nih.gov/grants/funding/ac_search_results.htm)\n",
    "* **`ADMINISTERING_IC`** - Administering Institute or Center - A two-character code to designate the agency, NIH Institute, or Center administering the grant. See [definitions]: [](http://grants.nih.gov/grants/glossary.htm#I14).\n",
    "* **`ARRA_FUNDED`** - “Y” indicates a project supported by funds appropriated through the American Recovery and Reinvestment Act of 2009.\n",
    "* **`ORG_NAME`** - The name of the educational institution, research organization, business, or government agency receiving funding for the grant, contract, cooperative agreement, or intramural project.  \n",
    "* **`ORG_DEPT`** - The departmental affiliation of the contact principal investigator for a project, using a standardized categorization of departments.  Names are available only for medical school departments.\n",
    "* **`STUDY_SECTION`** - A designator of the legislatively-mandated panel of subject matter experts that reviewed the research grant application for scientific and technical merit.\n",
    "* **`TOTAL_COST`** - Total project funding from all NIH Institute and Centers for a given fiscal year.\n",
    "* **`TOPIC_ID`** - Using text analysis techniques, a topic_id was assigned to each grant. This topic_id is a key for that topic. You can see what the topic contains by looking in the `topiclda_text` table in `umetricsgrants` database.\n",
    "* **`ED_INST_TYPE`** - Generic name for the grouping of components across an institution who has applied for or receives NIH funding.\n",
    "\n",
    "In the examples that follow, we'll only be using the columns in this table as predictors for models.  Once you get to the last exercise, however, when you are trying to make as accurate a model as you can, you will be free to use any of the columns in the table `nih_project` in the `umetricsgrants` database.  A complete description of all variables in that table: [http://exporter.nih.gov/about.aspx](http://exporter.nih.gov/about.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - descriptive statistics\n",
    "\n",
    "- Return to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Pandas provides some great functions for descriptive statistics ( [http://pandas.pydata.org/pandas-docs/stable/basics.html#descriptive-statistics](http://pandas.pydata.org/pandas-docs/stable/basics.html#descriptive-statistics) ).  Some examples:\n",
    "\n",
    "- **`describe()`** - \"computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course)\" ( from [http://pandas.pydata.org/pandas-docs/stable/basics.html#summarizing-data-describe](http://pandas.pydata.org/pandas-docs/stable/basics.html#summarizing-data-describe) )\n",
    "\n",
    "    - includes the count of values, mean, standard deviation, min, 25%, 50%, and 75% values, and the max.\n",
    "\n",
    "- **`head()` and `tail()`**, shown above - \"To view a small sample of a Series or DataFrame object, use the `head()` and `tail()` methods. The default number of elements to display is five, but you may pass a custom number.\" ( from [http://pandas.pydata.org/pandas-docs/stable/basics.html#head-and-tail](http://pandas.pydata.org/pandas-docs/stable/basics.html#head-and-tail) )\n",
    "- **`value_counts()`** - The `value_counts()` \"series method and top-level function computes a histogram of a one-dimensional array of values.\" ( from [http://pandas.pydata.org/pandas-docs/stable/basics.html#value-counts-histogramming-mode](http://pandas.pydata.org/pandas-docs/stable/basics.html#value-counts-histogramming-mode) ).  This method returns a Series of the counts of the number of times each unique value in the column is present in the column (also known as frequencies), from largest count to least, with the value itself the label for each row.\n",
    "\n",
    "For the first part of exercise 1, we will combine some of these methods for calculating descriptive statistics into a function that accepts a DataFrame of data from our `homework.MachineLearning2` table, then calculates and returns both the descriptives for the columns in the table and the top ten most frequently referenced departments.\n",
    "\n",
    "We will be making a function that returns multiple values to help you understand how this works in Python, since some of the machine learning functions used below return multiple values.\n",
    "\n",
    "In a Python function, if you want to return multiple values, you place each in the line with your return statement, separated by commas.  This is like a list of variables, but you don't need to put it in square brackets.  You just place the items after the `return` keyword.  So, if you wanted to write a function that returns the circumference and area of a circle when passed the radius of a circle (run the cell below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateCircleInfo( radius_IN ):\n",
    "    \n",
    "    '''\n",
    "    Accepts radius of circle.  Calculates circumference and area of circle,\n",
    "       returns them both.\n",
    "    '''\n",
    "    \n",
    "    # return references\n",
    "    circumference_OUT = -1\n",
    "    area_OUT = -1\n",
    "    \n",
    "    # got a radius?\n",
    "    if ( ( radius_IN is not None ) and ( radius_IN != \"\" ) and ( radius_IN > -1 ) ):\n",
    "        \n",
    "        # yes.  calculate circumference...\n",
    "        circumference_OUT = 2 * math.pi * radius_IN\n",
    "        \n",
    "        # ...and calculate area\n",
    "        area_OUT = math.pi * ( radius_IN ** 2 )\n",
    "        \n",
    "    #-- END check to see if radius is populated and not negative (we'll allow 0...). --#\n",
    "    \n",
    "    # return both diameter and area.\n",
    "    return circumference_OUT, area_OUT\n",
    "    \n",
    "#-- END function calculateCircleInfo() --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a function or method returns more than one thing, it returns the items in a tuple.  When you call a function or method that returns more than one thing, to accept all of the items it returns, you can either:\n",
    "\n",
    "- assign the results to a single variable.  This variable will contain a tuple, in which you can then reference the individual values returned by the function or method using square bracket notation, by index (starting with 0).\n",
    "- assign the results of the function to a list of the same number of variables, separated by commas, to the left of the assignment operator (the equal sign - \"=\").\n",
    "\n",
    "For example, to calculate the circumference and area of a circle with radius of 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[values] - For circle of radius 3: - circumference = 18.8495559215 - area = 28.2743338823\n",
      "[tuple] - For circle of radius 3: - circumference = 18.8495559215 - area = 28.2743338823\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "radius = -1\n",
    "circumference = -1\n",
    "area = -1\n",
    "circle_info_tuple = None\n",
    "\n",
    "# set radius\n",
    "radius = 3\n",
    "\n",
    "# calculate circumference and area - individual values\n",
    "circumference, area = calculateCircleInfo( radius )\n",
    "\n",
    "# print the results\n",
    "print( \"[values] - For circle of radius \" + str( radius ) + \": - circumference = \" + str( circumference ) + \" - area = \" + str( area ) ) \n",
    "\n",
    "# calculate circumference and area - tuple\n",
    "circle_info_tuple = calculateCircleInfo( radius )\n",
    "\n",
    "# unpack value using square-bracket tuple notation.\n",
    "circumference = circle_info_tuple[ 0 ] # first\n",
    "area = circle_info_tuple[ 1 ] # second\n",
    "\n",
    "# print the results\n",
    "print( \"[tuple] - For circle of radius \" + str( radius ) + \": - circumference = \" + str( circumference ) + \" - area = \" + str( area ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll make our own function that returns multiple values.  Complete the `printDescriptiveStats()` function below so that it uses the data frame of data from the `homework.MachineLearning2` table and uses it to create and return:\n",
    "\n",
    "- 1) - **`summary_OUT`** - the summary statistics for all variables in the dataset (the results of invoking `describe()`)\n",
    "- 2) - **`top_depts_OUT`** - a Series that contains the top 10 most referenced department names from the column \"`ORG_DEPT`\" and the count of mentions for each (use a combination of the `head()`, and `value_counts()` functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ML_Ex1",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def printDescriptiveStats( data_frame_IN ):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_frame_IN : A pandas DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    summary_OUT : A Pandas dataframe containing count, mean, standard deviation, \n",
    "              minimum, maximum, and the 25th, 50th and 75th percentile\n",
    "    top_depts_OUT : A pandas.core.series.Series containing the top 10 departments\n",
    "               and their frequencies\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    # use describe to summarize table\n",
    "    summary_OUT = data_frame_IN.describe()\n",
    "    \n",
    "    # get column \"ORG_DEPT\"\n",
    "    department_series = data_frame_IN[\"ORG_DEPT\"]\n",
    "\n",
    "    # calculate frequencies\n",
    "    department_frequencies = department_series.value_counts()\n",
    "    \n",
    "    # get top ten most frequent\n",
    "    top_depts_OUT = department_frequencies.head( 10 )\n",
    "    \n",
    "    return summary_OUT, top_depts_OUT \n",
    "\n",
    "    ### END SOLUTION\n",
    "    \n",
    "#-- END function printDescriptiveStats() --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell above that contains your function definition, then run the cell below to test it out and see what the data in data_frame looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ML_Ex1-autograde",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptives:\n",
      "       APPLICATION_ID          YEAR       topic_id    TOTAL_COST\n",
      "count    1.000000e+05  96805.000000  100000.000000  9.638000e+04\n",
      "mean     7.285785e+06   2006.668034     509.390420  3.546926e+05\n",
      "std      7.124825e+05      3.901659     295.025649  6.053330e+05\n",
      "min      1.000410e+05   1999.000000       0.000000  0.000000e+00\n",
      "25%      6.678821e+06   2004.000000     245.000000  1.497952e+05\n",
      "50%      7.216238e+06   2007.000000     520.000000  2.754790e+05\n",
      "75%      7.951276e+06   2010.000000     768.000000  3.679900e+05\n",
      "max      8.780827e+06   2013.000000     999.000000  5.599692e+07\n",
      "Top Departments:\n",
      "INTERNAL MEDICINE/MEDICINE     16192\n",
      "BIOCHEMISTRY                    6196\n",
      "NONE                            5236\n",
      "PHARMACOLOGY                    4735\n",
      "PSYCHIATRY                      4516\n",
      "BIOLOGY                         4494\n",
      "MICROBIOLOGY/IMMUN/VIROLOGY     4472\n",
      "PEDIATRICS                      3947\n",
      "PSYCHOLOGY                      3834\n",
      "PATHOLOGY                       3810\n",
      "Name: ORG_DEPT, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TEST - lets see what our data looks like\n",
    "output = printDescriptiveStats( data_frame )\n",
    "\n",
    "# TEST to make sure two things returned\n",
    "assert len( output ) == 2\n",
    "\n",
    "# get descriptives and top_departments\n",
    "descriptives = output[ 0 ]\n",
    "top_departments = output[ 1 ]\n",
    "\n",
    "print( \"Descriptives:\" )\n",
    "print( descriptives )\n",
    "print( \"Top Departments:\" )\n",
    "print( top_departments )\n",
    "\n",
    "# TEST to make sure frequencies are right.\n",
    "assert top_departments[ 0 ] == 16192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Subsetting Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Looking at the data, it is clear that there are missing values in `ORG_DEPT` (look at how many rows have \"NONE\", and where that values ranks in the frequencies for `ORG_DEPT`).\n",
    "\n",
    "In order to train a model to predict this value, we'll need to separate those that have a value from those that do now, so that we can train and evaluate our model using a samples of just those grant rows that contain an `ORG_DEPT` value.\n",
    "\n",
    "To filter out rows with no `ORG_DEPT` value, we'll need to do some basic cleaning of the data.  For that, we first need to figure out which variables have missing values.\n",
    "\n",
    "The function `calc_null_frequencies()` accepts a DataFrame in which you'd like to count null values per column.  It returns a DataFrame that lists each column in the DataFrame and for each, counts of non-null (\"False\") and null (\"True\") values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_null_frequencies( data_frame_IN ):\n",
    "\n",
    "    \"\"\"\n",
    "    for a given DataFrame, calculates how many values for \n",
    "    each variable are null and returns the resulting table.\n",
    "    \"\"\"\n",
    "    \n",
    "    # return reference\n",
    "    null_frequencies_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    data_frame_long = None\n",
    "    variable_value_series = None\n",
    "    is_null_value_series = None\n",
    "    variable_name_series = None\n",
    "    \n",
    "    # Use DataFrame.melt() to convert DataFrame table into long format\n",
    "    #   where every column value is its own row: variable names are in\n",
    "    #   column \"variable\", values are in column \"value\".\n",
    "    data_frame_long = pandas.melt( data_frame_IN )\n",
    "    \n",
    "    # get Series that just contains column values\n",
    "    #value_series = data_frame_long.value # alternate dot (\".\") notation\n",
    "    variable_value_series = data_frame_long[ \"value\" ]\n",
    "    \n",
    "    # Make a Series where position of each column value in Series\n",
    "    #    contains True if value was NULL or False if not.\n",
    "    is_null_value_series = variable_value_series.isnull()\n",
    "\n",
    "    # get Series of column names to match each of the values\n",
    "    #   (not distinct - one for each value - lots!)\n",
    "    # variable_name_series = data_frame_long.variable # alternate dot (\".\") notation\n",
    "    variable_name_series = data_frame_long[ \"variable\" ]\n",
    "    \n",
    "    # ceate a dataframe that sums counts of non-null values (\"False\") to\n",
    "    #    null values (\"True\") for each column name.\n",
    "    null_frequencies_OUT = pandas.crosstab( variable_name_series, is_null_value_series )\n",
    "\n",
    "    return null_frequencies_OUT\n",
    "\n",
    "#-- END function calc_null_frequencies() --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>value</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACTIVITY</th>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADMINISTERING_IC</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APPLICATION_ID</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARRA_FUNDED</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <td>96393</td>\n",
       "      <td>3607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ED_INST_TYPE</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_DEPT</th>\n",
       "      <td>99998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_NAME</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STUDY_SECTION</th>\n",
       "      <td>99829</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL_COST</th>\n",
       "      <td>96380</td>\n",
       "      <td>3620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>96805</td>\n",
       "      <td>3195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_id</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "value              False  True \n",
       "variable                       \n",
       "ACTIVITY           99999      1\n",
       "ADMINISTERING_IC  100000      0\n",
       "APPLICATION_ID    100000      0\n",
       "ARRA_FUNDED       100000      0\n",
       "CFDA_CODE          96393   3607\n",
       "ED_INST_TYPE      100000      0\n",
       "ORG_DEPT           99998      2\n",
       "ORG_NAME          100000      0\n",
       "STUDY_SECTION      99829    171\n",
       "TOTAL_COST         96380   3620\n",
       "YEAR               96805   3195\n",
       "topic_id          100000      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see what our NULL values look like\n",
    "calc_null_frequencies( data_frame )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have a better understanding of the issues our dataset has (in particular, `ORG_DEPT` has many NULL values and many rows where value is \"NONE\"), lets go ahead and write some code to deal with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - function `cleanData`\n",
    "\n",
    "- Return to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "To facilitate our cleanup, we will create a function that uses pandas' subsetting functionality to filter out rows where a given column contains a given value.\n",
    "\n",
    "In pandas, if you want to filter a DataFrame, you use a special form of square bracket notation to specify a boolean test to be used on each row to decide if it should be included in the resulting DataFrame.  If the test evaluates to True, the row is included.  If the test evaluates to False, the row is not included.\n",
    "\n",
    "The syntax for this notation is to place this boolean test inside square brackets next to the name of the variable that contains the DataFrame you want to filter:\n",
    "\n",
    "    filtered_data_frame = data_frame[ <boolean_test> ]\n",
    "\n",
    "Inside this `<boolean_test>`, you can reference the value for a given column using the standard square bracket notation (`data_frame[ \"<column_name>\" ]`), you can call functions or methods on objects, and you can use logical operators to `and` and `or` multiple tests together.  Examples make it clearer:\n",
    "\n",
    "    # filter out rows where column \"CFDA_CODE\" is not 123 (so keep rows where \"CFDA_CODE\" is 123)\n",
    "    filtered_data_frame = data_frame[ data_frame[ \"CFDA_CODE\" ] == 123 ]\n",
    "    \n",
    "    # filter out rows where column \"YEAR\" is NULL (so keep rows where pandas.notnull( data_frame[ \"YEAR\" ] == True )\n",
    "    filtered_data_frame = data_frame[ pandas.notnull( data_frame[ \"YEAR\" ] ) == True ]\n",
    "    \n",
    "    # filter out rows where column \"ORG_NAME\" = \"ARBYS\" (so keep rows where \"ORG_NAME\" is not \"ARBYS\")\n",
    "    filtered_data_frame = data_frame[ data_frame[ \"ORG_NAME\" ] != \"ARBYS\" ]\n",
    "    \n",
    "    # filter out rows where column \"ORG_NAME\" = \"ARBYS\" and \"YEAR\" is 1726 (Arby's didn't exist in 1726...)\n",
    "    filtered_data_frame = data_frame[ ( data_frame[ \"ORG_NAME\" ] != \"ARBYS\" ) and ( data_frame[ \"YEAR\" ] != 1726 )]\n",
    "\n",
    "In the cleanData function below, using the DataFrame (**`data_frame_IN`**), name of a column in the dataframe (**`column_name_IN`**), and filter value passed in (**`filter_value_IN`**):\n",
    "\n",
    "- return a cleaned dataframe that is a subset of **`data_frame_IN`** from which you've removed rows that contain the specified value (**`filter_value_IN`**) in the specified column (**`column_name_IN`**).  In order to return this subset, store it in the return variable **`cleaned_data_OUT`**.\n",
    "- Your function should be equipped to deal with NULL - a special value that can't be filtered normally.  The simplest way to do this is to decide that a certain string value will tell you when you are filtering on NULL (any case of the string \"NULL\" - \"NULL\"/\"null\"/\"NuLl\", etc.).  Whenever you find this value has been passed in `filter_value_IN`, use the `pandas.notnull()` function to test whether you should keep a given row, rather than a boolean operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ML_Ex2",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cleanData( data_frame_IN, column_name_IN, filter_value_IN ):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    - data_frame_IN : A pandas DataFrame\n",
    "    - column_name_IN : Name of the column on the dataframe\n",
    "    - filter_value_IN : The value that causes rows to be filtered out of data_frame_IN \n",
    "           if it is present in the column named column_name_IN.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - cleaned_data_OUT : A Pandas DataFrame containing only rows that did not have the \n",
    "                specified value in the specified column\n",
    "    \"\"\"\n",
    "    \n",
    "    # return reference\n",
    "    cleaned_data_OUT = None\n",
    "\n",
    "    # check to make sure that column name is in data frame's list of column names.\n",
    "    if( column_name_IN not in list( data_frame_IN.columns.values ) ):\n",
    "    \n",
    "        print(\"ERROR : Column you specified not present in the dataframe\")\n",
    "        clean_data_OUT = None\n",
    "        \n",
    "    #-- END check to see if column is in data frame's list of column names. --#\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    if filter_value_IN.upper() == \"NULL\":\n",
    "    \n",
    "        # keep rows where column passed is not NULL.\n",
    "        cleaned_data_OUT = data_frame_IN[ pandas.notnull( data_frame_IN[ column_name_IN ] ) == True ]\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # keep rows where column passed in does not contain filter_out_value_IN.\n",
    "        cleaned_data_OUT = data_frame_IN[ data_frame_IN[ column_name_IN ] != filter_value_IN ]\n",
    "\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return cleaned_data_OUT\n",
    "    \n",
    "#-- END function cleanData() --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, if the department name is NULL, we cannot use that data to train our classifier.  Fortunately, we can use our `cleanData()` function to remove all rows where \"`ORG_DEPT`\" is NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ML_Ex2-test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>value</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACTIVITY</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADMINISTERING_IC</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APPLICATION_ID</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARRA_FUNDED</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <td>96392</td>\n",
       "      <td>3606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ED_INST_TYPE</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_DEPT</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_NAME</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STUDY_SECTION</th>\n",
       "      <td>99828</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL_COST</th>\n",
       "      <td>96380</td>\n",
       "      <td>3618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>96804</td>\n",
       "      <td>3194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_id</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "value             False  True \n",
       "variable                      \n",
       "ACTIVITY          99998      0\n",
       "ADMINISTERING_IC  99998      0\n",
       "APPLICATION_ID    99998      0\n",
       "ARRA_FUNDED       99998      0\n",
       "CFDA_CODE         96392   3606\n",
       "ED_INST_TYPE      99998      0\n",
       "ORG_DEPT          99998      0\n",
       "ORG_NAME          99998      0\n",
       "STUDY_SECTION     99828    170\n",
       "TOTAL_COST        96380   3618\n",
       "YEAR              96804   3194\n",
       "topic_id          99998      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST - remove all rows where \"`ORG_DEPT`\" is NULL.\n",
    "cleaned_data_frame = cleanData( data_frame, \"ORG_DEPT\", \"null\" )\n",
    "\n",
    "# generate null frequencies data frame again to see if it worked.\n",
    "null_frequencies_df = calc_null_frequencies( cleaned_data_frame )\n",
    "\n",
    "# TEST - should be 402091 rows left after clearing out NULL departments.\n",
    "assert len( cleaned_data_frame ) == 99998\n",
    "\n",
    "# output NULL frequencies table\n",
    "null_frequencies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did clean most of the NULL values for us, even across all the columns.  There might be other values in \"`ORG_DEPT`\" we want to clear out as well, though (\"NONE\"), so lets run the code cell below to print a frequency table for \"`ORG_DEPT`\" and see what values remain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTERNAL MEDICINE/MEDICINE       16192\n",
       "BIOCHEMISTRY                      6196\n",
       "NONE                              5236\n",
       "PHARMACOLOGY                      4735\n",
       "PSYCHIATRY                        4516\n",
       "BIOLOGY                           4494\n",
       "MICROBIOLOGY/IMMUN/VIROLOGY       4472\n",
       "PEDIATRICS                        3947\n",
       "PSYCHOLOGY                        3834\n",
       "PATHOLOGY                         3810\n",
       "PHYSIOLOGY                        3534\n",
       "CHEMISTRY                         3523\n",
       "PUBLIC HEALTH &PREV MEDICINE      3403\n",
       "ANATOMY/CELL BIOLOGY              3219\n",
       "GENETICS                          2339\n",
       "NEUROLOGY                         2185\n",
       "SURGERY                           2043\n",
       "RADIATION-DIAGNOSTIC/ONCOLOGY     1990\n",
       "NEUROSCIENCES                     1852\n",
       "MISCELLANEOUS                     1815\n",
       "OTHER HEALTH PROFESSIONS          1539\n",
       "ENGINEERING (ALL TYPES)           1387\n",
       "OTHER BASIC SCIENCES              1295\n",
       "VETERINARY SCIENCES               1254\n",
       "OPHTHALMOLOGY                     1186\n",
       "DENTISTRY                         1100\n",
       "BIOSTATISTICS &OTHER MATH SCI     1064\n",
       "OBSTETRICS &GYNECOLOGY            1011\n",
       "BIOMEDICAL ENGINEERING            1000\n",
       "ANESTHESIOLOGY                     702\n",
       "OTOLARYNGOLOGY                     687\n",
       "NUTRITION                          580\n",
       "SOCIAL SCIENCES                    540\n",
       "NEUROSURGERY                       503\n",
       "DERMATOLOGY                        472\n",
       "FAMILY MEDICINE                    415\n",
       "ORTHOPEDICS                        385\n",
       "UROLOGY                            369\n",
       "PHYSICS                            274\n",
       "ZOOLOGY                            263\n",
       "PHYSICAL MEDICINE &REHAB           187\n",
       "ADMINISTRATION                     179\n",
       "EMERGENCY MEDICINE                 100\n",
       "BIOPHYSICS                          83\n",
       "OTHER CLINICAL SCIENCES             75\n",
       "NO CODE ASSIGNED                    13\n",
       "Name: ORG_DEPT, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_frame[\"ORG_DEPT\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of \"`NONE`\", \"`MISCELLANEOUS`\" and \"`NO CODE ASSIGNED`\" are also useless in terms of training or testing a machine learning model for predicting a grant's department.  Run the code cell below to use our `cleanData()` method to remove all rows where the \"`ORG_DEPT`\" column contains \"`NONE`\", \"`MISCELLANEOUS`\" or \"`NO CODE ASSIGNED`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting rid of NONE and NO CODE ASSIGNED.\n",
    "cleaned_data_frame = cleanData( cleaned_data_frame, \"ORG_DEPT\", \"NONE\" )\n",
    "cleaned_data_frame = cleanData( cleaned_data_frame, \"ORG_DEPT\", \"NO CODE ASSIGNED\" )\n",
    "cleaned_data_frame = cleanData( cleaned_data_frame, \"ORG_DEPT\", \"MISCELLANEOUS\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid confusion caused by mixed case, we should also make sure that the values of all categorical variables (variables of data type \"`numpy.object_`\") are converted to a uniform case.  Run the example code in the cell below to convert all categorical columns to upper case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>value</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACTIVITY</th>\n",
       "      <td>92934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADMINISTERING_IC</th>\n",
       "      <td>92934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APPLICATION_ID</th>\n",
       "      <td>92934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARRA_FUNDED</th>\n",
       "      <td>92934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <td>90053</td>\n",
       "      <td>2881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ED_INST_TYPE</th>\n",
       "      <td>92934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_DEPT</th>\n",
       "      <td>92934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_NAME</th>\n",
       "      <td>92934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STUDY_SECTION</th>\n",
       "      <td>92784</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL_COST</th>\n",
       "      <td>90034</td>\n",
       "      <td>2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>89952</td>\n",
       "      <td>2982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_id</th>\n",
       "      <td>92934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "value             False  True \n",
       "variable                      \n",
       "ACTIVITY          92934      0\n",
       "ADMINISTERING_IC  92934      0\n",
       "APPLICATION_ID    92934      0\n",
       "ARRA_FUNDED       92934      0\n",
       "CFDA_CODE         90053   2881\n",
       "ED_INST_TYPE      92934      0\n",
       "ORG_DEPT          92934      0\n",
       "ORG_NAME          92934      0\n",
       "STUDY_SECTION     92784    150\n",
       "TOTAL_COST        90034   2900\n",
       "YEAR              89952   2982\n",
       "topic_id          92934      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to upper case\n",
    "\n",
    "# loop over column names\n",
    "column_name_series = data_frame.columns.values\n",
    "for column_name in list( column_name_series ):\n",
    "    \n",
    "    # get column data type\n",
    "    column_data_type = cleaned_data_frame[ column_name ].dtype\n",
    "    \n",
    "    # is it categorical (numpy.object_)?\n",
    "    if column_data_type == numpy.object_:\n",
    "        \n",
    "        # yes - use Series.str.upper() method to convert all values in column to upper case.\n",
    "        cleaned_data_frame[ column_name ] = cleaned_data_frame[ column_name ].str.upper()\n",
    "        \n",
    "    #-- END check to see if variable is categorical --#\n",
    "    \n",
    "#-- END loop over columns. --#\n",
    "\n",
    "# and calculate the null frequencies again to see where we're at\n",
    "calc_null_frequencies( cleaned_data_frame )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, \"`YEAR`\", \"`STUDY_SECTION`\" and \"`CFDA_CODE`\" are categorical variables that we aren't interested in predicting, so run the cell below to remove rows where these columns are NULL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>value</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACTIVITY</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADMINISTERING_IC</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APPLICATION_ID</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARRA_FUNDED</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ED_INST_TYPE</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_DEPT</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_NAME</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STUDY_SECTION</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL_COST</th>\n",
       "      <td>86865</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_id</th>\n",
       "      <td>87068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "value             False  True \n",
       "variable                      \n",
       "ACTIVITY          87068      0\n",
       "ADMINISTERING_IC  87068      0\n",
       "APPLICATION_ID    87068      0\n",
       "ARRA_FUNDED       87068      0\n",
       "CFDA_CODE         87068      0\n",
       "ED_INST_TYPE      87068      0\n",
       "ORG_DEPT          87068      0\n",
       "ORG_NAME          87068      0\n",
       "STUDY_SECTION     87068      0\n",
       "TOTAL_COST        86865    203\n",
       "YEAR              87068      0\n",
       "topic_id          87068      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_frame = cleanData( cleaned_data_frame, \"CFDA_CODE\", \"NULL\" )\n",
    "cleaned_data_frame = cleanData( cleaned_data_frame, \"STUDY_SECTION\", \"NULL\" )\n",
    "cleaned_data_frame = cleanData( cleaned_data_frame, \"YEAR\", \"NULL\" )\n",
    "\n",
    "# Lets see if we have any more null frequencies to deal with\n",
    "calc_null_frequencies( cleaned_data_frame )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 5 missing values for \"`TOTAL_COST`\", which we'll want to address one way or another so we can use this column in our models.  We can get rid of them, but we could also use some sort of function to estimate a value (basic ones are centrality measures like mean, median or mode based on the other values in that column).  This can be complicated to do well.  Since there are a total of 49,080 records, losing 5 shouldn't impact the overall size of our data set, so for this exercise, we'll just delete these 5 records and move on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>value</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACTIVITY</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADMINISTERING_IC</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APPLICATION_ID</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARRA_FUNDED</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ED_INST_TYPE</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_DEPT</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG_NAME</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STUDY_SECTION</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL_COST</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_id</th>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "value             False\n",
       "variable               \n",
       "ACTIVITY          86865\n",
       "ADMINISTERING_IC  86865\n",
       "APPLICATION_ID    86865\n",
       "ARRA_FUNDED       86865\n",
       "CFDA_CODE         86865\n",
       "ED_INST_TYPE      86865\n",
       "ORG_DEPT          86865\n",
       "ORG_NAME          86865\n",
       "STUDY_SECTION     86865\n",
       "TOTAL_COST        86865\n",
       "YEAR              86865\n",
       "topic_id          86865"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of rows where \"TOTAL_COST\" is NULL.\n",
    "cleaned_data_frame = cleanData( cleaned_data_frame, \"TOTAL_COST\", \"NULL\" )\n",
    "\n",
    "# look at null frequencies now.\n",
    "calc_null_frequencies( cleaned_data_frame )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take another look at our ORG_DEPT variable, and see if we can combine some departments that are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERNAL MEDICINE/MEDICINE       15083\n",
      "BIOCHEMISTRY                      5758\n",
      "PHARMACOLOGY                      4468\n",
      "PSYCHIATRY                        4362\n",
      "MICROBIOLOGY/IMMUN/VIROLOGY       4275\n",
      "BIOLOGY                           4257\n",
      "PEDIATRICS                        3695\n",
      "PSYCHOLOGY                        3685\n",
      "PATHOLOGY                         3642\n",
      "PHYSIOLOGY                        3311\n",
      "CHEMISTRY                         3014\n",
      "PUBLIC HEALTH &PREV MEDICINE      2985\n",
      "ANATOMY/CELL BIOLOGY              2984\n",
      "GENETICS                          2235\n",
      "NEUROLOGY                         2082\n",
      "SURGERY                           1964\n",
      "RADIATION-DIAGNOSTIC/ONCOLOGY     1788\n",
      "NEUROSCIENCES                     1774\n",
      "OTHER HEALTH PROFESSIONS          1442\n",
      "ENGINEERING (ALL TYPES)           1294\n",
      "OTHER BASIC SCIENCES              1239\n",
      "OPHTHALMOLOGY                     1147\n",
      "VETERINARY SCIENCES               1086\n",
      "DENTISTRY                         1063\n",
      "BIOMEDICAL ENGINEERING             955\n",
      "BIOSTATISTICS &OTHER MATH SCI      946\n",
      "OBSTETRICS &GYNECOLOGY             926\n",
      "ANESTHESIOLOGY                     659\n",
      "OTOLARYNGOLOGY                     601\n",
      "NUTRITION                          558\n",
      "SOCIAL SCIENCES                    512\n",
      "NEUROSURGERY                       480\n",
      "DERMATOLOGY                        445\n",
      "ORTHOPEDICS                        371\n",
      "UROLOGY                            361\n",
      "FAMILY MEDICINE                    344\n",
      "PHYSICS                            261\n",
      "ZOOLOGY                            245\n",
      "PHYSICAL MEDICINE &REHAB           181\n",
      "ADMINISTRATION                     144\n",
      "EMERGENCY MEDICINE                  92\n",
      "BIOPHYSICS                          81\n",
      "OTHER CLINICAL SCIENCES             70\n",
      "Name: ORG_DEPT, dtype: int64\n",
      "Record Count: 86865\n"
     ]
    }
   ],
   "source": [
    "# Look at the department values that remain, and how many total records we have:\n",
    "print( cleaned_data_frame[ \"ORG_DEPT\" ].value_counts() )\n",
    "print( \"Record Count: \" + str( len( cleaned_data_frame ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It certainly looks like a lot of these departments can be combined into broader areas.  To reduce the number of categories to predict (and so make it easier for a model to make good predictions), run the cell below to combine some of these departments into broader areas of interest.\n",
    "\n",
    "The lines of code below that update \"`ORG_DEPT`\" use a similar syntax to those we used to filter out rows that contained a specified value in a given column.  The `.loc` reference is one of a number of special pandas ways to filter or index DataFrames and Series that has benefits over the base method indicated by using plain square brackets (these benefits are related to optimizations in pandas that are difficult to summarize - specifics on `.loc[]` and a discussion of optimizations: [http://pandas.pydata.org/pandas-docs/stable/indexing.html](http://pandas.pydata.org/pandas-docs/stable/indexing.html) ).\n",
    "\n",
    "Inside the brackets next to \"`.loc[]`\", there is a statement with two sections used to specify what row(s) - before the comma - and column(s) - after the comma - we are trying to reference.  Before the comma is an expression that indicates which rows to process that pandas calls a \"row_indexer\" - in this case, a boolean expression run on each row, where all rows that evaluate to `True` are included in the operation (looking for rows in which \"`ORG_DEPT`\" has a certain value).  After the comma is an expression that defines which columns to work with on the selected rows that pandas calls the \"column_indexer\" - in this case, it specifies a single column to update for rows included based on the row_indexer.\n",
    "\n",
    "So, in this example:\n",
    "\n",
    "    cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"ANESTHESIOLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "    \n",
    "- \"**`cleaned_data_frame[\"ORG_DEPT\"] == \"ANESTHESIOLOGY\"`**\" is the \"row_indexer\" test to select only rows where the value in \"`ORG_DEPT`\" is \"ANESTHESIOLOGY\".\n",
    "- \"**`, [ 'ORG_DEPT' ]`**\" is the \"column_indexer\" - we are working with column \"`ORG_DEPT`\", but only where it currently has a value of \"ANESTHESIOLOGY\".\n",
    "- then, after the reference to `.loc[]`, \"**` = \"MEDICINE\"`**\" tells the dataframe to set the value \"MEDICINE\" in the rows and columns selected by `.loc[]`.\n",
    "\n",
    "So, yeah.  That is what is going on.  Again, for more information, see [http://pandas.pydata.org/pandas-docs/stable/indexing.html](http://pandas.pydata.org/pandas-docs/stable/indexing.html), but feel free at this point to just run the cell below and move on.  =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDICINE                         39392\n",
      "BIOLOGY                          10797\n",
      "BIOCHEMISTRY                      5758\n",
      "OTHER HEALTH PROFESSIONS          4427\n",
      "PSYCHIATRY                        4362\n",
      "MICROBIOLOGY/IMMUN/VIROLOGY       4275\n",
      "PSYCHOLOGY                        3685\n",
      "CHEMISTRY                         3014\n",
      "ENGINEERING (ALL TYPES)           2249\n",
      "GENETICS                          2235\n",
      "NEUROSCIENCES                     1774\n",
      "OTHER BASIC SCIENCES              1239\n",
      "VETERINARY SCIENCES               1086\n",
      "BIOSTATISTICS &OTHER MATH SCI      946\n",
      "OTHER CLINICAL SCIENCES            628\n",
      "SOCIAL SCIENCES                    512\n",
      "PHYSICS                            261\n",
      "BIOPHYSICS                          81\n",
      "Name: ORG_DEPT, dtype: int64\n",
      "Record Count: 86721\n"
     ]
    }
   ],
   "source": [
    "# MEDICINE\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"ANESTHESIOLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"DENTISTRY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"DERMATOLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"EMERGENCY MEDICINE\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"FAMILY MEDICINE\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"INTERNAL MEDICINE/MEDICINE\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"NEUROLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"NEUROSURGERY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"OBSTETRICS &GYNECOLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"OPHTHALMOLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"ORTHOPEDICS\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"OTOLARYNGOLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"PATHOLOGY\",[ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"PHARMACOLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"PHYSICAL MEDICINE &REHAB\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"PEDIATRICS\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"PLASTIC SURGERY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"RADIATION-DIAGNOSTIC/ONCOLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"SURGERY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"UROLOGY\", [ 'ORG_DEPT' ] ] = \"MEDICINE\"\n",
    "\n",
    "# BIOLOGY\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"ANATOMY/CELL BIOLOGY\", [ 'ORG_DEPT' ] ] = \"BIOLOGY\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"PHYSIOLOGY\", [ 'ORG_DEPT' ] ] = \"BIOLOGY\"\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"ZOOLOGY\", [ 'ORG_DEPT' ] ] = \"BIOLOGY\"\n",
    "\n",
    "# ENGINEERING\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"BIOMEDICAL ENGINEERING\", [ 'ORG_DEPT' ] ] = \"ENGINEERING (ALL TYPES)\"\n",
    "\n",
    "# OTHER HEALTH PROFESSIONS\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"PUBLIC HEALTH &PREV MEDICINE\", [ 'ORG_DEPT' ] ] = \"OTHER HEALTH PROFESSIONS\"\n",
    "\n",
    "# OTHER CLINICAL SCIENCES\n",
    "cleaned_data_frame.loc[ cleaned_data_frame[\"ORG_DEPT\"] == \"NUTRITION\", [ 'ORG_DEPT' ] ] = \"OTHER CLINICAL SCIENCES\"\n",
    "\n",
    "# We will also get rid of \"ADMINISTRATION\n",
    "cleaned_data_frame = cleanData( cleaned_data_frame, \"ORG_DEPT\", \"ADMINISTRATION\" )\n",
    "\n",
    "# check out distribution of categories now:\n",
    "print( cleaned_data_frame[ \"ORG_DEPT\" ].value_counts() )\n",
    "print( \"Record Count: \" + str( len( cleaned_data_frame ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint - save `cleaned_data_frame`\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "At this point, our DataFrame referenced by `cleaned_data_frame` is as clean as we are going to make it for this assignment.  If you want to be able to pick up here in the future without having to run through all those steps again, you can write the DataFrame referenced by `cleaned_data_frame` to disk now, so if you need to start over, you can just pick up here without having to re-clean the data.  To write the dataframe to the file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This might be a good time to persist your dataframe to the file system,\n",
    "#    so you don't need to clean it again if you leave off.\n",
    "\n",
    "# set name of file you want to contain your data frame.\n",
    "dataframe_file_name = \"asgt7_cleaned.msg\"\n",
    "\n",
    "# write your dataframe out to the msgpack (binary JSON) format.\n",
    "cleaned_data_frame.to_msgpack( dataframe_file_name )\n",
    "\n",
    "# to load subsequently:\n",
    "#cleaned_data_frame = pandas.read_msgpack( dataframe_file_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Assessment\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we have a clean dataset, we can move on to the fun parts!! The python machine learning libraries do not accept categorical variables, so we need to convert all such variables to dummies first (boolean variables that capture the presence or absence in a given row of each of the potential values for each categorical variable). However, pandas makes it super easy! \n",
    "\n",
    "But before we do that, lets split our data variables into predictors (features, or dependent variables, or \"X\" variables) and variables to predict (independent variables, or \"Y\" variables).  For ease of reference, in subsequent examples, names of variables that pertain to predictors will start with \"`X_`\", and names of variables that pertain to variables we are to predict will start with \"`y_`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets go ahead and split into predictors and predicted\n",
    "\n",
    "# make a list of the column names not in dependent column name list (currently just \"ORG_DEPT\")\n",
    "# one line - predictor_column_list = [ column_name for column_name in list( cleaned_data_frame.columns.values ) if column_name not in [ \"ORG_DEPT\" ] ]\n",
    "X_column_list = []\n",
    "y_column_list = [ \"ORG_DEPT\" ]\n",
    "\n",
    "# loop over column names.\n",
    "column_name_list = cleaned_data_frame.columns.values\n",
    "for column_name in column_name_list:\n",
    "    \n",
    "    # if the name is not predicted_column_list, add it to predictor_column_list\n",
    "    if ( column_name not in y_column_list ):\n",
    "        \n",
    "        # add to the predictor_column_list\n",
    "        X_column_list.append( column_name )\n",
    "        \n",
    "    #-- END check to see if column is in predicted/IV/Y list --#\n",
    "    \n",
    "#-- END loop over columns. --#\n",
    "\n",
    "# split columns into two DataFrames, those we are to predict,\n",
    "#    and those that are predictors.\n",
    "X_data_frame = cleaned_data_frame[ X_column_list ]\n",
    "y_data_frame = cleaned_data_frame[ y_column_list ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can easily convert all categorical variables in `X_data_frame` into dummy/binary variables using the `pandas.get_dummies()` function ( [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python's sckikit algorithms dont work on categorical variables. Fortunately, Pandas provides an easy way out!\n",
    "X_data_frame = pandas.get_dummies( X_data_frame )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup - clear `cleaned_data_frame`\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "At this point, we have split `cleaned_data_frame` out into new DataFrames that hold just the X and y variables, and so we now have two separate and identical copies of our data loaded into memory - the original `cleaned_data_frame`, and the data in that DataFrame split into X and y variables.  Since there are two copies and we won't subsequently use the full data frame, we can clear the original `cleaned_data_frame` from memory.\n",
    "\n",
    "To do this, we'll empty the variable that references `cleaned_data_frame`, telling Python that the DataFrame instance is no longer needed, then we'll tell Python to garbage collect memory, removing the DataFrame from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the variable that references the DataFrame to None.\n",
    "cleaned_data_frame = None\n",
    "\n",
    "# Tell Python to free up memory.\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If we're building a model, we're going to need a way to know whether or not it's working. Convincing others of the quality of results is often the most challenging part of an analysis.  In machine learning, making repeatable, well-documented work with clear success metrics makes all the difference.\n",
    "\n",
    "For our classifier, we're going to use the following build methodology (Ghani, 2014):\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/demo-datasets/traintest.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, this methodology involves:\n",
    "\n",
    "- First [**splitting your data**](#1.-Split-data-into-training-and-testing-data-sets) into a training set (75% of your data) and a test set (25% of your data).\n",
    "- \"**Feature engineering** is the process of transforming raw data into features that better represent the underlying problem/data to the predictive models, resulting in improved model accuracy on unseen data.\" ( from [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) ).  In text, for example, this might involve deriving traits of the text like word counts, verb counts, or topics to feed into a model rather than simply giving it the raw text.\n",
    "- In the [**Model Build**](#2.-Model-Build---Fit-model) phase, you decide on a model then train or fit your model using your training data.\n",
    "- In the [**Evaluate Performance**](#3.-Evaluate-Performance---accuracy) phase, you run your fitted model on your set of testing predictors, then assess the quality of the model by comparing the predicted values to the actual values for each record in your testing data set. \n",
    "\n",
    "Since we have a limited number of relatively basic features, we won't be going into any Feature Engineering examples for this exercise.  However, feature engineering is an essential part of implementing quality machine learning - to learn more, start with the \"Discover Feature Engineering\" tutorial: [http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "\n",
    "Let us now split our dataset into test and training using the `train_test_split()` function from scikit learn's sklearn.cross_validation module ( [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html) ):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split data into training and testing data sets\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Let us now split our dataset into test and training using the `train_test_split()` function from scikit learn's sklearn.cross_validation module ( [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html) ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use train_test_split() to split our X and Y variables into separate 75% and 25%\n",
    "#    DataFrames of training (X_train and y_train) and testing (X_test and y_test) data.\n",
    "X_train, X_test, y_train, y_test = train_test_split( X_data_frame, y_data_frame, test_size = 0.25, random_state = 0 )\n",
    "\n",
    "# Before we fit the model, we also need to change the datatype of the y_train variable.\n",
    "# y_train currently is a Pandas Series, however, scikit-learn requires it to be a numpy array\n",
    "# So all we need to do is extract the raw values of y_train, and pass them onto scikit-learn\n",
    "y_train_values = y_train[ 'ORG_DEPT' ].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup - clear X and y data frames\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "At this point, we have split `X_data_frame` and `y_data_frame` out into new DataFrames that hold training and test data for X and y variables, and so we once again have two separate and identical copies of our data loaded into memory - the original `X_data_frame` and `y_data_frame`, and the data from those DataFrames split into training and test data.  Since there are two copies and we won't subsequently use `X_data_frame` and `y_data_frame`, we can clear them from memory.\n",
    "\n",
    "To do this, we'll empty the variables that reference `X_data_frame` and `y_data_frame`, telling Python that the DataFrame instances are no longer needed, then we'll tell Python to garbage collect memory, removing the DataFrames from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free up memory\n",
    "X_data_frame = None\n",
    "y_data_frame = None\n",
    "\n",
    "# Tell Python to free up memory.\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Build - Fit model\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python's `scikit-learn` is a very well known machine library. It is also well documented and maintained. You can learn all about it here: [http://scikit-learn.org/stable/](http://scikit-learn.org/stable/). We will be using different classifiers from this library for our predictions in this workbook. \n",
    "\n",
    "We will start with the simplest `LogisticRegression` model ( [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) ) and see how well that does.\n",
    "\n",
    "You can use any number of metrics to judge your models, but we will be using scikit learn's provided `accuracy_score()` (ratio of correct predictions to total number of predictions, based on comparing a set of predicted to values to a set of actual values) as our measure ( [http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Lets fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit( X_train, y_train_values )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the model, we see different parameters we can adjust as we refine the model based on running it against test data (values such as `intercept_scaling`, `max_iters`, `penalty`, and `solver`).  Example output:\n",
    "\n",
    "    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0)\n",
    "\n",
    "To adjust these parameters, one would alter the call that creates the `LogisticRegression()` model instance, passing it one or more of these parameters with a value other than the default.  So, to re-fit the model with `max_iter` of 1000, `intercept_scaling` of 2, and `solver` of \"lbfgs\" (pulled from thin air as an example), you'd create your model as follows:\n",
    "\n",
    "    model = LogisticRegression( max_iter = 1000, intercept_scaling = 2, solver = \"lbfgs\" )\n",
    "\n",
    "More details on what each of thee parameters mean is on the `LogisticRegression` documentation page: [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "The basic way one would tune these parameters is to iterate over fitting your model to your training data with different parameters (hopefully chosen based on your knowledge of your data and the model you are fitting), then testing it against training data until the model's accuracy is as high as you can get it.  Unfortunately, this exposes one to the potential that the model is over-fitted to one's test data, and so won't perform as well when it is used to predict other sets of data.\n",
    "\n",
    "Cross-validation is a good way to fine-tune the parameters with less risk of over-fitting.  It involves dividing your training data into 5 or so equal sets called folds, then choosing a different fold to serve as the test data set each time you test a new set of parameters. This sounds complicated, but scikit learn has functions to help, and a good tutorial on cross-validation can be found on the scikit learn site: [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Performance - accuracy\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.453576864536\n"
     ]
    }
   ],
   "source": [
    "# store our test \"to predict\" variables in \"expected\".\n",
    "expected = y_test\n",
    "\n",
    "# predict values from our \"predictors\" usin the model.\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# generate an accuracy score by comparing expected to predicted.\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an accuracy score of 0.45340... (45%). This is not a great score, however, it is much better than random guessing, which would have had a chance of 1/18 of succeeding. The other way to guess would be to take the mode, which in this case is MEDICINE with a frequency of 22497, which would give us an accuracy score of 22497/49013 = 45.9%. So logistic regression is about as good as just always assigning the mode when department is missing. Let's see if other classifiers can do any better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Train, fit and evaluate your model\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Complete the function below to train different classifiers from the scikit library.\n",
    "\n",
    "The `classifier()` function that you will implement:\n",
    "\n",
    "- Accepts X_train_IN, y_train_IN (should be type numpy.ndarray, not a Series or DataFrame - to convert, call \"`.values`\"), X_test_IN, and y_test_IN variables.\n",
    "- creates a model, fits the model, tests the model, and calculates an accuracy score for the model (like we did above).\n",
    "- returns the accuracy score as a percent (so a number between 1 and 100, not a decimal between 0 and 1 - ... so multiply by 100).\n",
    "\n",
    "Your goal is to come up with a classifier that gives at least 70% accuracy on the test dataset.  To do this, you can:\n",
    "\n",
    "- choose different models from those offered as part of scikit learn.\n",
    "\n",
    "    - To start, here are some resources to help with choosing a model:\n",
    "\n",
    "        - The scikit learn tutorial on choosing a model - [http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "        - This video on choosing and tuning a model - [http://blog.kaggle.com/2015/05/14/scikit-learn-video-5-choosing-a-machine-learning-model/](http://blog.kaggle.com/2015/05/14/scikit-learn-video-5-choosing-a-machine-learning-model/)\n",
    "    \n",
    "    - In particular, here are some sets of models you could explore from the \"predicting a category\" with \"labeled data\" branch of the [scikit learn tutorial on choosing a model](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) (just make sure your data doesn't violate the assumptions of the model and that the model you choose is appropriate for your data - predicting a categorical variable using a mix of numeric and dummied categorical data):\n",
    "\n",
    "        - other Linear Models like the `LogisticRegression` - [http://scikit-learn.org/stable/modules/linear_model.htmlFeel](http://scikit-learn.org/stable/modules/linear_model.html)\n",
    "        - Decision Tree models - [http://scikit-learn.org/stable/modules/tree.html](http://scikit-learn.org/stable/modules/tree.html)\n",
    "        - Ensemble classifiers - [http://scikit-learn.org/stable/modules/ensemble.html](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "        - Nearest neighbors classifiers - [http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification](http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)\n",
    "        - Stocahstic Gradient Descent - [http://scikit-learn.org/stable/modules/sgd.html#classification](http://scikit-learn.org/stable/modules/sgd.html#classification)\n",
    "        - Kernel Approximation + SGDClassifier - [http://scikit-learn.org/stable/modules/kernel_approximation.html](http://scikit-learn.org/stable/modules/kernel_approximation.html)\n",
    "\n",
    "- play around with different parameters for the models you try.\n",
    "- experiment with different sets of X variables.\n",
    "- _Advanced_ - You can try starting again from the top with an SQL query that uses JOINs to pull in columns from other tables, to add more variables to your pool of available predictors.\n",
    "- _Advanced_ - You could also try to derive additional features from the data present in your query and add those features to your predictors.\n",
    "\n",
    "Again, in general, make sure that the model and parameters you choose are appropriate for both your X and Y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ML_Ex3",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def classifier(X_train_IN, y_train_IN, X_test_IN, y_test_IN):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train_IN : A pandas DataFrame of features used for training the classifier\n",
    "    y_train_IN : A numpy array of y values used for training the classifier\n",
    "    X_test_IN, y_test_IN : Use these to test the accuracy of your classifier\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    accuracy score : a float giving the percent (0 to 100) of accurate predictions you made\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    # declare variables\n",
    "    y_train_type = None\n",
    "    y_train_array = None\n",
    "    my_accuracy_score = -1\n",
    "   \n",
    "    # check to see if y_train_IN is either a Series or a DataFrame\n",
    "    y_train_type = type( y_train_IN )\n",
    "    if ( ( y_train_type == pandas.core.series.Series ) or ( y_train_type == pandas.core.frame.DataFrame ) ):\n",
    "        \n",
    "        # Series or DataFrame - convert to pandas array\n",
    "        y_train_array = y_train_IN.values\n",
    "        \n",
    "    elif ( y_train_type == numpy.ndarray ):\n",
    "        \n",
    "        # this is what it should be.\n",
    "        y_train_array = y_train_IN\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # not Series or DataFrame numpy ndarray - just use it and see what happens...?\n",
    "        print( \"Unexpected y_train_IN type: \" + str( y_train_type ) )\n",
    "        y_train_array = y_train_IN\n",
    "        \n",
    "    #-- END check to see if y_train_IN is wrong type. --#\n",
    "    \n",
    "    # for fitting model, use y_train_array rather than y_train_IN.\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train_IN, y_train_array)\n",
    "    expected = y_test_IN\n",
    "    predicted = model.predict(X_test_IN)\n",
    "    \n",
    "    # get accuracy score\n",
    "    my_accuracy_score = accuracy_score( expected, predicted )\n",
    "    \n",
    "    # * 100 to turn into percentage value\n",
    "    my_accuracy_score = my_accuracy_score * 100\n",
    "    \n",
    "    return my_accuracy_score\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ML_Ex3-test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Percentage: 71.9062773857\n"
     ]
    }
   ],
   "source": [
    "# Remember,  first extract the values of y_train, before calling the classifier function\n",
    "y_train_values = y_train[\"ORG_DEPT\"].values\n",
    "\n",
    "# TEST to see if your accuracy is greater than 70%. This might take several minutes to run!!!\n",
    "test_accuracy_score = classifier( X_train, y_train_values, X_test, y_test )\n",
    "print( \"Accuracy Percentage: \" + str( test_accuracy_score ) )\n",
    "\n",
    "# TEST - is it greater than 70%?\n",
    "assert test_accuracy_score >= 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Links to documentation:\n",
    "\n",
    "* [Scikit-Learn Documentation](#http://scikit-learn.org/stable/)\n",
    "* [NIH Reporter Documentation](#http://exporter.nih.gov/about.aspx)\n",
    "* Ghani, Rayid (2014), 2- Case-Study.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
